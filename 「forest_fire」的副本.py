# -*- coding: utf-8 -*-
"""„Äåforest fire„ÄçÁöÑÂâØÊú¨

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cuEMj0ckGsPwIqFr8dCzEM8yge8UgNHa
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
elmadafri_the_wildfire_dataset_path = kagglehub.dataset_download('elmadafri/the-wildfire-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input

from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import PIL.Image

train_dir = '/kaggle/input/the-wildfire-dataset/the_wildfire_dataset_2n_version/test'
val_dir = '/kaggle/input/the-wildfire-dataset/the_wildfire_dataset_2n_version/train'
test_dir = '/kaggle/input/the-wildfire-dataset/the_wildfire_dataset_2n_version/val'

img_height, img_width = 224, 224
batch_size = 32

train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=20,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(train_dir, target_size=(img_height, img_width),
                                               batch_size=batch_size, class_mode='categorical')

val_data = val_datagen.flow_from_directory(val_dir, target_size=(img_height, img_width),
                                           batch_size=batch_size, class_mode='categorical')

test_data = test_datagen.flow_from_directory(test_dir, target_size=(img_height, img_width),
                                             batch_size=batch_size, class_mode='categorical')

classes = os.listdir(train_dir)
num_classes = len(classes)

# Display the class names
print(f'Number of Classes: {num_classes}')
print(f'Classes: {classes}')

model = Sequential([
    Input(shape=(img_height, img_width, 3)),
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_data,
                    validation_data=val_data,
                    epochs=10)

loss, acc = model.evaluate(test_data)
print(f'Test Accuracy: {acc*100:.2f}%')

model.save('forest_fire_detector.h5')
model.save_weights('forest_fire_detector.weights.h5')

!pip install streamlit
import streamlit as st

@st.cache_resource
def download_and_load_model():
    model_path = "forest_fire_detector.h5"
    if not os.path.exists(model_path):
        file_id = "1AbCdEfGhIjKl"  # Replace with your actual file ID
        gdown.download(f"https://drive.google.com/uc?id=1sQjGhSczD1sIRTMM-OTuk9qcIbddUHwd", model_path, quiet=False)
    return load_model(model_path)

model = download_and_load_model()

#Load your trained model
model = load_model('forest_fire_detector.h5')

# Define your class labels (same as in training)
class_names = ['fire', 'no_fire']  # Update if you have different labels

# Streamlit app UI
st.set_page_config(page_title="Forest Fire Detector", layout="centered")
st.title("üî• Forest Fire Detection App üå≤")
st.write("Upload an image to check if it shows signs of a forest fire.")

uploaded_file = st.file_uploader("Upload Image", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    img = Image.open(uploaded_file).convert('RGB')
    st.image(img, caption='Uploaded Image', use_column_width=True)

    # Preprocess the image
    img = img.resize((224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # Predict
    prediction = model.predict(img_array)
    class_index = np.argmax(prediction)
    predicted_label = class_names[class_index]
    confidence = prediction[0][class_index] * 100

    # Show result
    st.write(f"### Prediction: `{predicted_label.upper()}`")
    st.write(f"Confidence: `{confidence:.2f}%`")

    if predicted_label == 'fire':
        st.error("‚ö†Ô∏è Fire Detected! Take action immediately!")
    else:
        st.success("‚úÖ No Fire Detected. Environment seems safe.")